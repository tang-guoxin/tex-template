\begin{thebibliography}{10}

\bibitem{lenet5nn}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{wang2021gnn}
Kuan Wang, Yuyu Zhang, Diyi Yang, Le~Song, and Tao Qin.
\newblock Gnn is a counter? revisiting gnn for question answering.
\newblock {\em arXiv preprint arXiv:2110.03192}, 2021.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{wang2018dataset}
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei~A Efros.
\newblock Dataset distillation.
\newblock {\em arXiv preprint arXiv:1811.10959}, 2018.

\bibitem{courbariaux2015binaryconnect}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{li2016ternary}
Fengfu Li, Bo~Zhang, and Bin Liu.
\newblock Ternary weight networks.
\newblock {\em arXiv preprint arXiv:1605.04711}, 2016.

\bibitem{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock {\em arXiv preprint arXiv:1510.00149}, 2015.

\bibitem{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock {\em arXiv preprint arXiv:1308.3432}, 2013.

\bibitem{qin2020binary}
Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu
  Sebe.
\newblock Binary neural networks: A survey.
\newblock {\em Pattern Recognition}, 105:107281, 2020.

\bibitem{1998Reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock Reinforcement learning.
\newblock {\em A Bradford Book}, volume 15(7):665--685, 1998.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock Q-learning.
\newblock {\em Machine learning}, 8(3):279--292, 1992.

\bibitem{jaques2017tuning}
Natasha Jaques, Shixiang Gu, Richard~E Turner, and Douglas Eck.
\newblock Tuning recurrent neural networks with reinforcement learning.
\newblock 2017.

\bibitem{baker2016designing}
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar.
\newblock Designing neural network architectures using reinforcement learning.
\newblock {\em arXiv preprint arXiv:1611.02167}, 2016.

\bibitem{zoph2016neural}
Barret Zoph and Quoc~V Le.
\newblock Neural architecture search with reinforcement learning.
\newblock {\em arXiv preprint arXiv:1611.01578}, 2016.

\bibitem{aenugu2020training}
Sneha Aenugu.
\newblock Training spiking neural networks using reinforcement learning.
\newblock {\em arXiv preprint arXiv:2005.05941}, 2020.

\bibitem{lindsay2020novel}
James Lindsay and Sidney Gigivi.
\newblock A novel way of training a neural network with reinforcement learning
  and without back propagation.
\newblock In {\em 2020 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--6. IEEE, 2020.

\bibitem{lazarus2022deep}
Christopher Lazarus and Mykel~J Kochenderfer.
\newblock Deep binary reinforcement learning for scalable verification.
\newblock {\em arXiv preprint arXiv:2203.05704}, 2022.

\bibitem{2017Training}
S.~Bose and M.~Huber.
\newblock Training neural networks with policy gradient.
\newblock In {\em International Joint Conference on Neural Networks (IJCNN)},
  2017.

\end{thebibliography}
